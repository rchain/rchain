import functools
import re
import os
import queue
import shlex
import string
import time
import shutil
import logging
from logging import (Logger)
import threading
from threading import Event
import contextlib
from multiprocessing import Queue, Process
from typing import (
    Dict,
    List,
    Tuple,
    Optional,
    Generator,
    AbstractSet,
    Set
)
from dataclasses import dataclass
import requests
from rchain.client import RClient, RClientException
from rchain.pb.DeployServiceCommon_pb2 import LightBlockInfo, BlockInfo
from rchain.crypto import PrivateKey
from rchain.certificate import get_node_id_raw
from rchain.const import DEFAULT_PHLO_LIMIT, DEFAULT_PHLO_PRICE
from cryptography.hazmat.primitives.serialization import load_pem_private_key
from cryptography.hazmat.backends import default_backend
from docker.client import DockerClient
from docker.models.containers import Container
from docker.models.containers import ExecResult

from .common import (
    make_tempdir,
    make_tempfile,
    TestingContext,
    NonZeroExitCodeError,
    GetBlockError,
    ParsingError,
    SynchronyConstraintError,
    NotAnActiveValidatorError,
    ValidatorNotContainsLatestMes
)
from .wait import (
    wait_for_node_started,
    wait_for_approved_block_received_handler_state,
)

from .error import(
    RNodeAddressNotFoundError,
    CommandTimeoutError,
)

from .utils import (
    parse_mvdag_str,
    ISLINUX,
    get_free_tcp_port
)

DEFAULT_IMAGE = os.environ.get("DEFAULT_IMAGE", "rchain-integration-tests:latest")
_PB_REPEATED_STR_SEP = "#$"

rnode_binary = '/opt/docker/bin/rnode'
rnode_directory = "/var/lib/rnode"
rnode_deploy_dir = "{}/deploy".format(rnode_directory)
rnode_bonds_file = '{}/genesis/bonds.txt'.format(rnode_directory)
rnode_wallets_file = '{}/genesis/wallets.txt'.format(rnode_directory)
rnode_certificate_path = '{}/node.certificate.pem'.format(rnode_directory)
rnode_key_path = '{}/node.key.pem'.format(rnode_directory)
rnode_logback_file = '{}/logback.xml'.format(rnode_directory)
rnode_default_launcher_args = [
    # We don't want the launcher script (generated by sbt-native-packager) to
    # swallow first java error and exit with confusing "No java installation was
    # detected" message.
    '-no-version-check',
]
rnode_default_client_options = [
    # Make client calls on internal port which expose all APIs
    '--grpc-port=40402',
]

default_http_port = 40403
default_internal_grpc_port = 40402
default_external_grpc_port = 40401

@dataclass
class PortMapping:
    http: int
    external_grpc: int
    internal_grpc: int

class Node:
    def __init__(self, *, container: Container, deploy_dir: str, command_timeout: int, network: str, ports: Optional[PortMapping]) -> None:
        self.container = container
        self.local_deploy_dir = deploy_dir
        self.remote_deploy_dir = rnode_deploy_dir
        self.name = container.name
        self.command_timeout = command_timeout
        self.network = network
        self.terminate_background_logging_event = threading.Event()
        self.background_logging = LoggingThread(
            container=container,
            logger=logging.getLogger('peers'),
            terminate_thread_event=self.terminate_background_logging_event,
        )
        self.background_logging.setDaemon(True)
        self.background_logging.start()
        self.ports = ports

    def __repr__(self) -> str:
        return '<Node(name={})>'.format(repr(self.name))

    def get_node_pem_cert(self) -> bytes:
        return self.shell_out("cat", rnode_certificate_path).encode('utf8')

    def get_node_pem_key(self) -> bytes:
        return self.shell_out("cat", rnode_key_path).encode('utf8')

    def view_file(self, path: str) -> str:
        return self.shell_out("cat", path)

    def get_node_id_raw(self) -> bytes:
        key = load_pem_private_key(self.get_node_pem_key(), None, default_backend())
        return get_node_id_raw(key)

    def logs(self) -> str:
        return self.container.logs().decode('utf-8')

    def get_rnode_address(self) -> str:
        log_content = self.logs()
        regex = "Listening for traffic on (rnode://.+@{name}\\?protocol=\\d+&discovery=\\d+)\\.$"\
            .format(name=self.container.name)
        match = re.search(regex, log_content, re.MULTILINE | re.DOTALL)
        if match is None:
            raise RNodeAddressNotFoundError(regex)
        address = match.group(1)
        return address

    def get_metrics(self) -> str:
        resp = requests.get("http://{}:{}/metrics".format(self.get_self_host(), self.get_http_port()))
        return resp.content.decode('utf8')

    def get_connected_peers_metric_value(self) -> str:
        try:
            resp = requests.get("http://{}:{}/metrics".format(self.get_self_host(), self.get_http_port()))
            result = ''
            for line in resp.content.decode('utf8').splitlines():
                if line.startswith("rchain_comm_rp_connect_peers"):
                    result = line
                    break
            return result
        except NonZeroExitCodeError as e:
            if e.exit_code == 1:
                return ''
            raise

    def get_peer_node_ip(self, network_name: str) -> str:
        self.container.reload()
        network_config = self.container.attrs['NetworkSettings']['Networks'][network_name]
        assert network_config is not None
        return network_config['IPAddress']

    def get_self_host(self) -> str:
        if ISLINUX:
            return self.get_peer_node_ip(self.network)
        return 'localhost'

    def get_http_port(self) -> int:
        if ISLINUX:
            return default_http_port
        assert self.ports
        return self.ports.http

    def get_external_grpc_port(self) -> int:
        if ISLINUX:
            return default_external_grpc_port
        assert self.ports
        return self.ports.external_grpc

    def get_internal_grpc_port(self) -> int:
        if ISLINUX:
            return default_internal_grpc_port
        assert self.ports
        return self.ports.internal_grpc

    def cleanup(self) -> None:
        self.container.remove(force=True, v=True)
        self.terminate_background_logging_event.set()
        self.background_logging.join()

    def get_blocks_count(self, depth: int) -> int:
        show_blocks = self.get_blocks(depth)
        return len(show_blocks)

    def get_blocks(self, depth: int) -> List[LightBlockInfo]:
        with RClient(self.get_self_host(), self.get_external_grpc_port()) as client:
            return client.show_blocks(depth)

    def get_block(self, hash: str) -> BlockInfo:
        with RClient(self.get_self_host(), self.get_external_grpc_port()) as client:
            try:
                return client.show_block(hash)
            except RClientException as e:
                message = e.args[0]
                raise GetBlockError(('show-block',), 1, message) from e

    # Too low level -- do not use directly.  Prefer shell_out() instead.
    def _exec_run_with_timeout(self, cmd: Tuple[str, ...], stderr: bool = True) -> Tuple[int, str]:
        control_queue: queue.Queue = Queue(1)

        def command_process() -> None:
            exec_result: ExecResult = self.container.exec_run(cmd, stderr=stderr)
            control_queue.put((exec_result.exit_code, exec_result.output.decode('utf-8')))

        process = Process(target=command_process)
        logging.info("COMMAND {} {}".format(self.name, cmd))
        process.start()

        try:
            exit_code, output = control_queue.get(True, self.command_timeout)
        except queue.Empty as e:
            raise CommandTimeoutError(cmd, self.command_timeout) from e
        finally:
            process.terminate()

        if exit_code != 0:
            for line in output.splitlines():
                logging.info('{}: {}'.format(self.name, line))
            logging.warning("EXITED {} {} {}".format(self.name, cmd, exit_code))
        else:
            for line in output.splitlines():
                logging.debug('{}: {}'.format(self.name, line))
            logging.debug("EXITED {} {} {}".format(self.name, cmd, exit_code))
        return exit_code, output

    def shell_out(self, *cmd: str, stderr: bool = True) -> str:
        exit_code, output = self._exec_run_with_timeout(cmd, stderr=stderr)
        if exit_code != 0:
            raise NonZeroExitCodeError(command=cmd, exit_code=exit_code, output=output)
        return output

    def rnode_command(self, *node_args: str, stderr: bool = True) -> str:
        return self.shell_out(rnode_binary, *rnode_default_client_options,
                              *rnode_default_launcher_args, *node_args, stderr=stderr)

    def eval(self, rho_file_path: str) -> str:
        return self.rnode_command('eval', rho_file_path)

    def deploy(self, rho_file_path: str, private_key: PrivateKey, phlo_limit:int = DEFAULT_PHLO_LIMIT,
               phlo_price: int = DEFAULT_PHLO_PRICE, valid_after_block_no:int=0) -> str:
        try:
            now_time = int(time.time()*1000)
            with RClient(self.get_self_host(), self.get_external_grpc_port()) as client:
                return client.deploy(private_key, self.view_file(rho_file_path), phlo_price, phlo_limit, valid_after_block_no, now_time)
        except RClientException as e:
            message = e.args[0]
            if "Parsing error" in message:
                raise ParsingError(command=("propose", ), exit_code=1, output=message) from e
            # TODO out of phlogiston error
            raise e

    def get_vdag(self) -> str:
        return self.rnode_command('vdag', stderr=False)

    def get_mvdag(self) -> str:
        return self.rnode_command('mvdag', stderr=False)

    def get_parsed_mvdag(self) -> Dict[str, Set[str]]:
        return parse_mvdag_str(self.get_mvdag())

    def deploy_string(self, rholang_code: str, private_key: PrivateKey, phlo_limit:int = DEFAULT_PHLO_LIMIT,
                      phlo_price: int = DEFAULT_PHLO_PRICE, valid_after_block_no:int = 0) -> str:
        try:
            now_time = int(time.time()*1000)
            with RClient(self.get_self_host(), self.get_external_grpc_port()) as client:
                return client.deploy(private_key, rholang_code, phlo_price, phlo_limit, valid_after_block_no, now_time)
        except RClientException as e:
            message = e.args[0]
            if "Parsing error" in message:
                raise ParsingError(command=("propose", ), exit_code=1, output=message) from e
            # TODO out of phlogiston error
            raise e

    def find_deploy(self, deploy_id: str) -> LightBlockInfo:
        with RClient(self.get_self_host(), self.get_external_grpc_port()) as client:
            return client.find_deploy(deploy_id)

    def propose(self) -> str:
        try:
            with RClient(self.get_self_host(), self.get_internal_grpc_port()) as client:
                return client.propose()
        except RClientException as e:
            message = e.args[0]
            if "Must wait for more blocks from other validators" in message:
                raise SynchronyConstraintError(command=('propose',), exit_code=1, output=message) from e
            if "ReadOnlyMode" in message:
                raise NotAnActiveValidatorError(command=('propose',), exit_code=1, output=message) from e
            if "Validator does not have a latest message" in message:
                raise ValidatorNotContainsLatestMes(command=('propose',), exit_code=1, output=message) from e
            raise e

    def last_finalized_block(self) -> BlockInfo:
        with RClient(self.get_self_host(), self.get_external_grpc_port()) as client:
            return client.last_finalized_block()

    def repl(self, rholang_code: str, stderr: bool = False) -> str:
        quoted_rholang_code = shlex.quote(rholang_code)
        output = self.shell_out(
            'sh',
            '-c',
            'echo {quoted_rholang_code} | {rnode_binary} {rnode_default_client_options} repl'
                .format(quoted_rholang_code=quoted_rholang_code,rnode_binary=rnode_binary,
                        rnode_default_client_options=" ".join(rnode_default_client_options)),
            stderr=stderr,
        )
        return output

    def cat_forward_file(self, public_key: str) -> str:
        return self.shell_out('cat', '/opt/docker/forward_{}.rho'.format(public_key))

    def cat_bond_file(self, public_key: str) -> str:
        return self.shell_out('cat', '/opt/docker/bond_{}.rho'.format(public_key))

    __timestamp_rx = "\\d\\d:\\d\\d:\\d\\d\\.\\d\\d\\d"
    __log_message_rx = re.compile("^{timestamp_rx} (.*?)(?={timestamp_rx})"
                                  .format(timestamp_rx=__timestamp_rx), re.MULTILINE | re.DOTALL)

    def log_lines(self) -> List[str]:
        log_content = self.logs()
        return Node.__log_message_rx.split(log_content)

    def deploy_contract_with_substitution(self, substitute_dict: Dict[str, str], rho_file_path: str,
                                          private_key: PrivateKey, phlo_limit:int = DEFAULT_PHLO_LIMIT,
                                          phlo_price: int = DEFAULT_PHLO_PRICE) -> str:
        """
        Supposed that you have a contract with content like below.

        new x in { x!("#DATA") }

        If you pass a dict {'#DATA': "123456"} as substitute_dict args in this func,
        this method would substitute the string #DATA in the contract with 123456, which turns out to be

        new x in { x!("123456") }

        And then deploy the contract in the node
        """
        shutil.copyfile(rho_file_path, os.path.join(self.local_deploy_dir, os.path.basename(rho_file_path)))
        container_contract_file_path = os.path.join(self.remote_deploy_dir, os.path.basename(rho_file_path))
        substitute_rules = ';'.join([r's/{}/{}/g'.format(key.replace(r'/', r'\/'), value.replace(r'/', r'\/'))
                                     for key, value in substitute_dict.items()])
        self.shell_out(
            'sed',
            '-i',
            '-e', substitute_rules,
            container_contract_file_path,
        )
        self.deploy(container_contract_file_path, private_key, phlo_limit, phlo_price)
        block_hash = self.propose()
        return block_hash


class LoggingThread(threading.Thread):
    def __init__(self, terminate_thread_event: Event, container: Container, logger: Logger) -> None:
        super().__init__()
        self.terminate_thread_event = terminate_thread_event
        self.container = container
        self.logger = logger

    def run(self) -> None:
        containers_log_lines_generator = self.container.logs(stream=True, follow=True)
        try:
            while True:
                if self.terminate_thread_event.is_set():
                    break
                line = next(containers_log_lines_generator)
                self.logger.info('{:>11}: {}'.format(self.container.name[-11:], line.decode('utf-8').rstrip()))
        except StopIteration:
            pass


class DeployThread(threading.Thread):
    def __init__(self, name: str, node: Node, contract: str, count: int, private_key: PrivateKey) -> None:
        threading.Thread.__init__(self)
        self.name = name
        self.node = node
        self.contract = contract
        self.count = count
        self.private_key = private_key

    def run(self) -> None:
        for _ in range(self.count):
            self.node.deploy(self.contract, self.private_key)
            self.node.propose()


def make_container_command(container_command: str, container_command_flags: AbstractSet,
                           container_command_options: Dict) -> str:
    opts = ['{}={}'.format(option, argument) for option, argument in container_command_options.items()]
    flags = ' '.join(container_command_flags)
    result = '{} {} {}'.format(container_command, flags, ' '.join(opts))
    return result


def make_node( # pylint: disable=too-many-locals
    *,
    docker_client: DockerClient,
    name: str,
    network: str,
    bonds_file: str,
    container_command: str,
    container_command_flags: AbstractSet,
    container_command_options: Dict,
    command_timeout: int,
    extra_volumes: Optional[List[str]],
    allowed_peers: Optional[List[str]],
    image: str = DEFAULT_IMAGE,
    mem_limit: Optional[str] = None,
    wallets_file: Optional[str] = None,
) -> Node:
    assert isinstance(name, str)
    assert '_' not in name, 'Underscore is not allowed in host name'
    deploy_dir = make_tempdir("rchain-integration-test")

    hosts_allow_file_content = \
        "ALL:ALL" if allowed_peers is None else "\n".join("ALL: {}".format(peer) for peer in allowed_peers)

    hosts_allow_file = make_tempfile("hosts-allow-{}".format(name), hosts_allow_file_content)
    hosts_deny_file = make_tempfile("hosts-deny-{}".format(name), "ALL: ALL")
    container_command_options["-Dlogback.configurationFile"] = rnode_logback_file

    command = make_container_command(container_command,
                                     container_command_flags,
                                     container_command_options)

    env = {}
    java_options = os.environ.get('_JAVA_OPTIONS')
    if java_options is not None:
        env['_JAVA_OPTIONS'] = java_options
    logging.debug('Using _JAVA_OPTIONS: {}'.format(java_options))

    volumes = [
        "{}:/etc/hosts.allow".format(hosts_allow_file),
        "{}:/etc/hosts.deny".format(hosts_deny_file),
        "{}:{}".format(bonds_file, rnode_bonds_file),
        "{}:{}".format(deploy_dir, rnode_deploy_dir),
        "{}:{}".format(os.path.abspath("resources/logback.xml"), rnode_logback_file)
    ]

    if wallets_file is not None:
        volumes.append('{}:{}'.format(wallets_file, rnode_wallets_file))
    if extra_volumes:
        all_volumes = volumes + extra_volumes
    else:
        all_volumes = volumes

    if ISLINUX:
        ports = None
        port_map = None
    else:
        port_map = PortMapping(http=get_free_tcp_port(), external_grpc=get_free_tcp_port(), internal_grpc=get_free_tcp_port())
        ports = {default_http_port: port_map.http,
                 default_external_grpc_port: port_map.external_grpc,
                 default_internal_grpc_port: port_map.internal_grpc}

    logging.info('STARTING %s %s', name, command)
    container = docker_client.containers.run(
        image,
        name=name,
        user='root',
        detach=True,
        mem_limit=mem_limit,
        network=network,
        volumes=all_volumes,
        command=command,
        hostname=name,
        environment=env,
        ports=ports
    )

    node = Node(
        container=container,
        deploy_dir=deploy_dir,
        command_timeout=command_timeout,
        network=network,
        ports=port_map
    )

    return node


def make_bootstrap_node(
    *,
    docker_client: DockerClient,
    network: str,
    bonds_file: str,
    private_key: PrivateKey,
    command_timeout: int,
    allowed_peers: Optional[List[str]] = None,
    mem_limit: Optional[str] = None,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    wallets_file: Optional[str] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0,
    max_peer_queue_size: int = 10,
    give_up_after_skipped: int = 0,
    drop_peer_after_retries: int = 0,
    number_of_active_validators: int = 10,
    epoch_length: int = 10000,
    quarantine_length: int = 50000,
    min_phlo_price: int = 1
) -> Node:

    container_name = make_bootstrap_name(network)

    container_command_flags = set([
        *rnode_default_launcher_args,
        "--standalone",
        "--prometheus",
        "--no-upnp",
        "--allow-private-addresses"
    ])

    container_command_options = {
        "--protocol-port":                           40400,
        "--validator-private-key":          private_key.to_hex(),
        "--validator-public-key":           private_key.get_public_key().to_hex(),
        "--host":                           container_name,
        "--synchrony-constraint-threshold": synchrony_constraint_threshold,
        "--frrd-max-peer-queue-size":            max_peer_queue_size,
        "--frrd-give-up-after-skipped":          give_up_after_skipped,
        "--frrd-drop-peer-after-retries":        drop_peer_after_retries,
        "--number-of-active-validators":    number_of_active_validators,
        "--epoch-length":                   epoch_length,
        "--quarantine-length":              quarantine_length,
        "--min-phlo-price":                 min_phlo_price
    }

    if cli_flags is not None:
        container_command_flags.update(cli_flags)

    if cli_options is not None:
        container_command_options.update(cli_options)


    container = make_node(
        docker_client=docker_client,
        name=container_name,
        network=network,
        bonds_file=bonds_file,
        container_command='run',
        container_command_flags=container_command_flags,
        container_command_options=container_command_options,
        command_timeout=command_timeout,
        extra_volumes=extra_volumes,
        allowed_peers=allowed_peers,
        mem_limit=mem_limit if mem_limit is not None else '4G',
        wallets_file=wallets_file,
    )
    return container


def make_container_name(network_name: str, name: str) -> str:
    return "{network_name}.{name}".format(network_name=network_name, name=name)


def make_bootstrap_name(network_name: str) -> str:
    return make_container_name(network_name=network_name, name='bootstrap')


def make_peer_name(network_name: str, name: str) -> str:
    if name.isdigit():
        actual_name = 'peer{}'.format(name)
    else:
        actual_name = name
    return make_container_name(network_name=network_name, name=actual_name)


def make_peer(
    *,
    docker_client: DockerClient,
    network: str,
    name: str,
    bonds_file: str,
    command_timeout: int,
    bootstrap: Node,
    private_key: PrivateKey,
    allowed_peers: Optional[List[str]] = None,
    mem_limit: Optional[str] = None,
    wallets_file: Optional[str] = None,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0,
    max_peer_queue_size: int = 10,
    give_up_after_skipped: int = 0,
    drop_peer_after_retries: int = 0,
    number_of_active_validators: int = 10,
    epoch_length: int = 10000,
    quarantine_length: int = 50000
) -> Node:
    assert isinstance(name, str)
    assert '_' not in name, 'Underscore is not allowed in host name'
    name = make_peer_name(network, name)

    bootstrap_address = bootstrap.get_rnode_address()

    container_command_flags = set([
        "--prometheus",
        "--no-upnp",
        "--allow-private-addresses"
    ])

    if cli_flags is not None:
        container_command_flags.update(cli_flags)

    container_command_options = {
        "--bootstrap":                      bootstrap_address,
        "--validator-private-key":          private_key.to_hex(),
        "--validator-public-key":           private_key.get_public_key().to_hex(),
        "--host":                           name,
        "--synchrony-constraint-threshold": synchrony_constraint_threshold,
        "--frrd-max-peer-queue-size":            max_peer_queue_size,
        "--frrd-give-up-after-skipped":          give_up_after_skipped,
        "--frrd-drop-peer-after-retries":        drop_peer_after_retries,
        "--number-of-active-validators":    number_of_active_validators,
        "--epoch-length":                   epoch_length,
        "--quarantine-length":              quarantine_length
    }

    if cli_options is not None:
        container_command_options.update(cli_options)

    container = make_node(
        docker_client=docker_client,
        name=name,
        network=network,
        bonds_file=bonds_file,
        container_command='run',
        container_command_flags=container_command_flags,
        container_command_options=container_command_options,
        command_timeout=command_timeout,
        extra_volumes=extra_volumes,
        allowed_peers=allowed_peers,
        mem_limit=mem_limit if not None else '4G',
        wallets_file=wallets_file,
    )
    return container


@contextlib.contextmanager
def started_peer(
    *,
    context: TestingContext,
    network: str,
    name: str,
    bootstrap: Node,
    private_key: PrivateKey,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0,
    epoch_length: int = 10000,
    quarantine_length: int = 50000
) -> Generator[Node, None, None]:
    peer = make_peer(
        docker_client=context.docker,
        network=network,
        name=name,
        bonds_file=context.bonds_file,
        bootstrap=bootstrap,
        private_key=private_key,
        command_timeout=context.command_timeout,
        wallets_file=context.wallets_file,
        cli_flags=cli_flags,
        cli_options=cli_options,
        extra_volumes=extra_volumes,
        synchrony_constraint_threshold=synchrony_constraint_threshold,
        epoch_length=epoch_length,
        quarantine_length=quarantine_length
    )
    try:
        wait_for_node_started(context, peer)
        yield peer
    finally:
        peer.cleanup()


@contextlib.contextmanager
def bootstrap_connected_peer(
    *,
    context: TestingContext,
    bootstrap: Node,
    name: str,
    private_key: PrivateKey,
    cli_options: Optional[Dict[str, str]] = None,
    synchrony_constraint_threshold: float = 0.0,
    epoch_length: int = 10000,
    quarantine_length: int = 50000
) -> Generator[Node, None, None]:
    with started_peer(
        context=context,
        network=bootstrap.network,
        name=name,
        bootstrap=bootstrap,
        private_key=private_key,
        cli_options=cli_options,
        synchrony_constraint_threshold=synchrony_constraint_threshold,
        epoch_length=epoch_length,
        quarantine_length=quarantine_length
    ) as peer:
        wait_for_approved_block_received_handler_state(context, peer)
        yield peer


def make_random_network_name(context: TestingContext, length: int) -> str:
    return ''.join(context.random_generator.choice(string.ascii_lowercase) for m in range(length))


@contextlib.contextmanager
def docker_network(context: TestingContext, docker_client: DockerClient) -> Generator[str, None, None]:
    network_name = "rchain-{}".format(make_random_network_name(context, 5))
    docker_client.networks.create(network_name, driver="bridge")
    try:
        yield network_name
    finally:
        for network in docker_client.networks.list():
            if network_name == network.name:
                network.remove()


@contextlib.contextmanager
def started_bootstrap(
    *,
    context: TestingContext,
    network: str,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict[str, str]] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0,
    epoch_length: int = 10000,
    quarantine_length: int = 50000,
    min_phlo_price: int = 1
) -> Generator[Node, None, None]:
    bootstrap_node = make_bootstrap_node(
        docker_client=context.docker,
        network=network,
        bonds_file=context.bonds_file,
        private_key=context.bootstrap_key,
        command_timeout=context.command_timeout,
        cli_flags=cli_flags,
        cli_options=cli_options,
        wallets_file=context.wallets_file,
        extra_volumes=extra_volumes,
        synchrony_constraint_threshold=synchrony_constraint_threshold,
        epoch_length=epoch_length,
        quarantine_length=quarantine_length,
        min_phlo_price=min_phlo_price
    )
    try:
        wait_for_node_started(context, bootstrap_node)
        yield bootstrap_node
    finally:
        bootstrap_node.cleanup()


@contextlib.contextmanager
def started_bootstrap_with_network(
    context: TestingContext,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    synchrony_constraint_threshold: float = 0.0,
    epoch_length: int = 10000,
    quarantine_length: int = 50000,
    min_phlo_price: int = 1,
    extra_volumes: Optional[List[str]] = None,
    wait_for_approved_block: bool = False,
) -> Generator[Node, None, None]:
    with docker_network(context, context.docker) as network:
        with started_bootstrap(
                context=context,
                network=network,
                cli_flags=cli_flags,
                cli_options=cli_options,
                synchrony_constraint_threshold=synchrony_constraint_threshold,
                extra_volumes=extra_volumes,
                epoch_length=epoch_length,
                quarantine_length=quarantine_length,
                min_phlo_price=min_phlo_price
        ) as bootstrap:
            if wait_for_approved_block:
                wait_for_approved_block_received_handler_state(context, bootstrap)
            yield bootstrap

ready_bootstrap_with_network = functools.partial(started_bootstrap_with_network,
        wait_for_approved_block=True)
