import functools
import re
import os
import queue
import shlex
import string
import shutil
import logging
from logging import (Logger)
import threading
from threading import Event
import contextlib
from multiprocessing import Queue, Process
from typing import (
    Dict,
    List,
    Tuple,
    Optional,
    Generator,
    AbstractSet,
    Set
)
from rchain.crypto import PrivateKey
from rchain.certificate import get_node_id_raw
from rchain.vault import DEFAULT_PHLO_LIMIT, DEFAULT_PHLO_PRICE
from cryptography.hazmat.primitives.serialization import load_pem_private_key
from cryptography.hazmat.backends import default_backend
from docker.client import DockerClient
from docker.models.containers import Container
from docker.models.containers import ExecResult

from .common import (
    make_tempdir,
    make_tempfile,
    TestingContext,
    NonZeroExitCodeError,
    GetBlockError,
    ParsingError,
    SynchronyConstraintError
)
from .wait import (
    wait_for_node_started,
    wait_for_approved_block_received_handler_state,
)

from .error import(
    RNodeAddressNotFoundError,
    CommandTimeoutError,
)

from .utils import (
    extract_block_count_from_show_blocks,
    parse_show_block_output,
    parse_show_blocks_output,
    extract_block_hash_from_propose_output,
    extract_deploy_id_from_deploy_output,
    parse_mvdag_str,
    BlockInfo,
    LightBlockInfo
)

DEFAULT_IMAGE = os.environ.get("DEFAULT_IMAGE", "rchain-integration-tests:latest")
_PB_REPEATED_STR_SEP = "#$"

rnode_binary = '/opt/docker/bin/rnode'
rnode_directory = "/var/lib/rnode"
rnode_deploy_dir = "{}/deploy".format(rnode_directory)
rnode_bonds_file = '{}/genesis/bonds.txt'.format(rnode_directory)
rnode_wallets_file = '{}/genesis/wallets.txt'.format(rnode_directory)
rnode_certificate_path = '{}/node.certificate.pem'.format(rnode_directory)
rnode_key_path = '{}/node.key.pem'.format(rnode_directory)
rnode_default_launcher_args = [
    # We don't want the launcher script (generated by sbt-native-packager) to
    # swallow first java error and exit with confusing "No java installation was
    # detected" message.
    '-no-version-check',
]

class Node:
    def __init__(self, *, container: Container, deploy_dir: str, command_timeout: int, network: str) -> None:
        self.container = container
        self.local_deploy_dir = deploy_dir
        self.remote_deploy_dir = rnode_deploy_dir
        self.name = container.name
        self.command_timeout = command_timeout
        self.network = network
        self.terminate_background_logging_event = threading.Event()
        self.background_logging = LoggingThread(
            container=container,
            logger=logging.getLogger('peers'),
            terminate_thread_event=self.terminate_background_logging_event,
        )
        self.background_logging.setDaemon(True)
        self.background_logging.start()

    def __repr__(self) -> str:
        return '<Node(name={})>'.format(repr(self.name))

    def get_node_pem_cert(self) -> bytes:
        return self.shell_out("cat", rnode_certificate_path).encode('utf8')

    def get_node_pem_key(self) -> bytes:
        return self.shell_out("cat", rnode_key_path).encode('utf8')

    def get_node_id_raw(self) -> bytes:
        key = load_pem_private_key(self.get_node_pem_key(), None, default_backend())
        return get_node_id_raw(key)

    def logs(self) -> str:
        return self.container.logs().decode('utf-8')

    def get_rnode_address(self) -> str:
        log_content = self.logs()
        regex = "Listening for traffic on (rnode://.+@{name}\\?protocol=\\d+&discovery=\\d+)\\.$".format(name=self.container.name)
        match = re.search(regex, log_content, re.MULTILINE | re.DOTALL)
        if match is None:
            raise RNodeAddressNotFoundError(regex)
        address = match.group(1)
        return address

    def get_metrics(self) -> str:
        return self.shell_out('curl', '-s', 'http://localhost:40403/metrics')

    def get_connected_peers_metric_value(self) -> str:
        try:
            return self.shell_out('sh', '-c', 'curl -s http://localhost:40403/metrics | grep ^rchain_comm_rp_connect_peers\\ ')
        except NonZeroExitCodeError as e:
            if e.exit_code == 1:
                return ''
            raise

    def get_peer_node_ip(self, network_name: str) -> str:
        self.container.reload()
        network_config = self.container.attrs['NetworkSettings']['Networks'][network_name]
        assert network_config is not None
        return network_config['IPAddress']

    def cleanup(self) -> None:
        self.container.remove(force=True, v=True)
        self.terminate_background_logging_event.set()
        self.background_logging.join()

    def show_blocks_with_depth(self, depth: int) -> str:
        return self.rnode_command('show-blocks', '--depth', str(depth), stderr=False)

    def show_block(self, hash: str) -> str:
        return self.rnode_command('show-block', hash, stderr=False)

    def get_blocks_count(self, depth: int) -> int:
        show_blocks_output = self.show_blocks_with_depth(depth)
        return extract_block_count_from_show_blocks(show_blocks_output)

    def show_blocks_parsed(self, depth: int) -> List[LightBlockInfo]:
        show_blocks_output = self.show_blocks_with_depth(depth)
        return parse_show_blocks_output(show_blocks_output)

    def show_block_parsed(self, hash: str) -> BlockInfo:
        show_block_output = self.show_block(hash)
        block_info = parse_show_block_output(show_block_output)
        return block_info

    def get_block(self, block_hash: str) -> str:
        try:
            return self.rnode_command('show-block', block_hash, stderr=False)
        except NonZeroExitCodeError as e:
            raise GetBlockError(command=e.command, exit_code=e.exit_code, output=e.output)

    # Too low level -- do not use directly.  Prefer shell_out() instead.
    def _exec_run_with_timeout(self, cmd: Tuple[str, ...], stderr: bool = True) -> Tuple[int, str]:
        control_queue: queue.Queue = Queue(1)

        def command_process() -> None:
            exec_result: ExecResult = self.container.exec_run(cmd, stderr=stderr)
            control_queue.put((exec_result.exit_code, exec_result.output.decode('utf-8')))

        process = Process(target=command_process)
        logging.info("COMMAND {} {}".format(self.name, cmd))
        process.start()

        try:
            exit_code, output = control_queue.get(True, self.command_timeout)
        except queue.Empty:
            raise CommandTimeoutError(cmd, self.command_timeout)
        finally:
            process.terminate()

        if exit_code != 0:
            for line in output.splitlines():
                logging.info('{}: {}'.format(self.name, line))
            logging.warning("EXITED {} {} {}".format(self.name, cmd, exit_code))
        else:
            for line in output.splitlines():
                logging.debug('{}: {}'.format(self.name, line))
            logging.debug("EXITED {} {} {}".format(self.name, cmd, exit_code))
        return exit_code, output

    def shell_out(self, *cmd: str, stderr: bool = True) -> str:
        exit_code, output = self._exec_run_with_timeout(cmd, stderr=stderr)
        if exit_code != 0:
            raise NonZeroExitCodeError(command=cmd, exit_code=exit_code, output=output)
        return output

    def rnode_command(self, *node_args: str, stderr: bool = True) -> str:
        return self.shell_out(rnode_binary, *rnode_default_launcher_args, *node_args, stderr=stderr)

    def eval(self, rho_file_path: str) -> str:
        return self.rnode_command('eval', rho_file_path)

    def deploy(self, rho_file_path: str, private_key: PrivateKey, phlo_limit:int = DEFAULT_PHLO_LIMIT, phlo_price: int = DEFAULT_PHLO_PRICE) -> str:
        try:
            output = self.rnode_command('deploy', '--private-key={}'.format(private_key.to_hex()), '--phlo-limit={}'.format(phlo_limit), '--phlo-price={}'.format(phlo_price), rho_file_path, stderr=False)
            deploy_id = extract_deploy_id_from_deploy_output(output)
            return deploy_id
        except NonZeroExitCodeError as e:
            if "Parsing error" in e.output:
                raise ParsingError(command=e.command, exit_code=e.exit_code, output=e.output)
            # TODO out of phlogiston error
            raise e

    def get_vdag(self) -> str:
        return self.rnode_command('vdag', stderr=False)

    def get_mvdag(self) -> str:
        return self.rnode_command('mvdag', stderr=False)

    def get_parsed_mvdag(self) -> Dict[str, Set[str]]:
        return parse_mvdag_str(self.get_mvdag())

    def deploy_string(self, rholang_code: str, private_key: str, phlo_limit:int = DEFAULT_PHLO_LIMIT, phlo_price: int = DEFAULT_PHLO_PRICE) -> str:
        quoted_rholang = shlex.quote(rholang_code)
        deploy_out = self.shell_out('sh', '-c', 'echo {quoted_rholang} >/tmp/deploy_string.rho && {rnode_binary} deploy --private-key={private_key} --phlo-limit={phlo_limit} --phlo-price={phlo_price} /tmp/deploy_string.rho'.format(
            rnode_binary=rnode_binary,
            quoted_rholang=quoted_rholang,
            private_key=private_key,
            phlo_limit=phlo_limit,
            phlo_price=phlo_price
        ), stderr=False)
        return extract_deploy_id_from_deploy_output(deploy_out)

    def find_deploy(self, deploy_id: str) -> LightBlockInfo:
        output = self.rnode_command("find-deploy", "--deploy-id", deploy_id, stderr=False)
        block_info = parse_show_blocks_output(output)
        return block_info[0]

    def propose(self) -> str:
        try:
            output = self.rnode_command('propose', stderr=False)
            block_hash = extract_block_hash_from_propose_output(output)
            return block_hash
        except NonZeroExitCodeError as e:
            if "Must wait for more blocks from other validators" in e.output:
                raise SynchronyConstraintError(command=e.command, exit_code=e.exit_code, output=e.output)
            raise e

    def last_finalized_block(self) -> BlockInfo:
        output = self.rnode_command('last-finalized-block', stderr=False)
        block_info = parse_show_block_output(output)
        return block_info

    def repl(self, rholang_code: str, stderr: bool = False) -> str:
        quoted_rholang_code = shlex.quote(rholang_code)
        output = self.shell_out(
            'sh',
            '-c',
            'echo {quoted_rholang_code} | {rnode_binary} repl'.format(quoted_rholang_code=quoted_rholang_code,rnode_binary=rnode_binary),
            stderr=stderr,
        )
        return output

    def cat_forward_file(self, public_key: str) -> str:
        return self.shell_out('cat', '/opt/docker/forward_{}.rho'.format(public_key))

    def cat_bond_file(self, public_key: str) -> str:
        return self.shell_out('cat', '/opt/docker/bond_{}.rho'.format(public_key))

    __timestamp_rx = "\\d\\d:\\d\\d:\\d\\d\\.\\d\\d\\d"
    __log_message_rx = re.compile("^{timestamp_rx} (.*?)(?={timestamp_rx})".format(timestamp_rx=__timestamp_rx), re.MULTILINE | re.DOTALL)

    def log_lines(self) -> List[str]:
        log_content = self.logs()
        return Node.__log_message_rx.split(log_content)

    def deploy_contract_with_substitution(self, substitute_dict: Dict[str, str], rho_file_path: str, private_key: PrivateKey, phlo_limit:int = DEFAULT_PHLO_LIMIT, phlo_price: int = DEFAULT_PHLO_PRICE) -> str:
        """
        Supposed that you have a contract with content like below.

        new x in { x!("#DATA") }

        If you pass a dict {'#DATA': "123456"} as substitute_dict args in this func,
        this method would substitute the string #DATA in the contract with 123456, which turns out to be

        new x in { x!("123456") }

        And then deploy the contract in the node
        """
        shutil.copyfile(rho_file_path, os.path.join(self.local_deploy_dir, os.path.basename(rho_file_path)))
        container_contract_file_path = os.path.join(self.remote_deploy_dir, os.path.basename(rho_file_path))
        substitute_rules = ';'.join([r's/{}/{}/g'.format(key.replace(r'/', r'\/'), value.replace(r'/', r'\/')) for key, value in substitute_dict.items()])
        self.shell_out(
            'sed',
            '-i',
            '-e', substitute_rules,
            container_contract_file_path,
        )
        self.deploy(container_contract_file_path, private_key, phlo_limit, phlo_price)
        block_hash = self.propose()
        return block_hash


class LoggingThread(threading.Thread):
    def __init__(self, terminate_thread_event: Event, container: Container, logger: Logger) -> None:
        super().__init__()
        self.terminate_thread_event = terminate_thread_event
        self.container = container
        self.logger = logger

    def run(self) -> None:
        containers_log_lines_generator = self.container.logs(stream=True, follow=True)
        try:
            while True:
                if self.terminate_thread_event.is_set():
                    break
                line = next(containers_log_lines_generator)
                self.logger.info('{}: {}'.format(self.container.name, line.decode('utf-8').rstrip()))
        except StopIteration:
            pass


class DeployThread(threading.Thread):
    def __init__(self, name: str, node: Node, contract: str, count: int, private_key: PrivateKey) -> None:
        threading.Thread.__init__(self)
        self.name = name
        self.node = node
        self.contract = contract
        self.count = count
        self.private_key = private_key

    def run(self) -> None:
        for _ in range(self.count):
            self.node.deploy(self.contract, self.private_key)
            self.node.propose()


def make_container_command(container_command: str, container_command_flags: AbstractSet, container_command_options: Dict) -> str:
    opts = ['{} {}'.format(option, argument) for option, argument in container_command_options.items()]
    flags = ' '.join(container_command_flags)
    result = '{} {} {}'.format(container_command, flags, ' '.join(opts))
    return result


def make_node(
    *,
    docker_client: DockerClient,
    name: str,
    network: str,
    bonds_file: str,
    container_command: str,
    container_command_flags: AbstractSet,
    container_command_options: Dict,
    command_timeout: int,
    extra_volumes: Optional[List[str]],
    allowed_peers: Optional[List[str]],
    image: str = DEFAULT_IMAGE,
    mem_limit: Optional[str] = None,
    wallets_file: Optional[str] = None,
) -> Node:
    assert isinstance(name, str)
    assert '_' not in name, 'Underscore is not allowed in host name'
    deploy_dir = make_tempdir("rchain-integration-test")

    hosts_allow_file_content = \
        "ALL:ALL" if allowed_peers is None else "\n".join("ALL: {}".format(peer) for peer in allowed_peers)

    hosts_allow_file = make_tempfile("hosts-allow-{}".format(name), hosts_allow_file_content)
    hosts_deny_file = make_tempfile("hosts-deny-{}".format(name), "ALL: ALL")

    command = make_container_command(container_command, container_command_flags, container_command_options)

    env = {}
    java_options = os.environ.get('_JAVA_OPTIONS')
    if java_options is not None:
        env['_JAVA_OPTIONS'] = java_options
    logging.debug('Using _JAVA_OPTIONS: {}'.format(java_options))

    volumes = [
        "{}:/etc/hosts.allow".format(hosts_allow_file),
        "{}:/etc/hosts.deny".format(hosts_deny_file),
        "{}:{}".format(bonds_file, rnode_bonds_file),
        "{}:{}".format(deploy_dir, rnode_deploy_dir),
    ]

    if wallets_file is not None:
        volumes.append('{}:{}'.format(wallets_file, rnode_wallets_file))
    if extra_volumes:
        all_volumes = volumes + extra_volumes
    else:
        all_volumes = volumes

    logging.info('STARTING %s %s', name, command)
    container = docker_client.containers.run(
        image,
        name=name,
        user='root',
        detach=True,
        mem_limit=mem_limit,
        network=network,
        volumes=all_volumes,
        command=command,
        hostname=name,
        environment=env,
    )

    node = Node(
        container=container,
        deploy_dir=deploy_dir,
        command_timeout=command_timeout,
        network=network,
    )

    return node


def get_absolute_path_for_mounting(relative_path: str, mount_dir: Optional[str]=None)-> str:
    """Drone runs each job in a new Docker container FOO. That Docker
    container has a new filesystem. Anything in that container can read
    anything in that filesystem. To read files from HOST, it has to be shared
    though, so let's share /tmp:/tmp. You also want to start new Docker
    containers, so you share /var/run/docker.sock:/var/run/docker.sock. When
    you start a new Docker container from FOO, it's not in any way nested. You
    just contact the Docker daemon running on HOST via the shared docker.sock.
    So when you start a new image from FOO, the HOST creates a new Docker
    container BAR with brand new filesystem. So if you tell Docker from FOO to
    mount /MOUNT_DIR:/MOUNT_DIR from FOO to BAR, the Docker daemon will actually mount
    /MOUNT_DIR from HOST to BAR, and not from FOO to BAR.
    """

    if mount_dir is not None:
        return os.path.join(mount_dir, relative_path)
    return os.path.abspath(os.path.join('resources', relative_path))


def make_bootstrap_node(
    *,
    docker_client: DockerClient,
    network: str,
    bonds_file: str,
    private_key: PrivateKey,
    command_timeout: int,
    allowed_peers: Optional[List[str]] = None,
    mem_limit: Optional[str] = None,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    wallets_file: Optional[str] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0,
    max_peer_queue_size: int = 10,
    give_up_after_skipped: int = 0,
    drop_peer_after_retries: int = 0
) -> Node:

    container_name = make_bootstrap_name(network)

    container_command_flags = set([
        *rnode_default_launcher_args,
        "--standalone",
        "--prometheus",
        "--no-upnp",
        "--allow-private-addresses"
    ])

    container_command_options = {
        "--port":                           40400,
        "--validator-private-key":          private_key.to_hex(),
        "--validator-public-key":           private_key.get_public_key().to_hex(),
        "--host":                           container_name,
        "--synchrony-constraint-threshold": synchrony_constraint_threshold,
        "--max-peer-queue-size":            max_peer_queue_size,
        "--give-up-after-skipped":          give_up_after_skipped,
        "--drop-peer-after-retries":        drop_peer_after_retries
    }

    if cli_flags is not None:
        container_command_flags.update(cli_flags)

    if cli_options is not None:
        container_command_options.update(cli_options)


    container = make_node(
        docker_client=docker_client,
        name=container_name,
        network=network,
        bonds_file=bonds_file,
        container_command='run',
        container_command_flags=container_command_flags,
        container_command_options=container_command_options,
        command_timeout=command_timeout,
        extra_volumes=extra_volumes,
        allowed_peers=allowed_peers,
        mem_limit=mem_limit if mem_limit is not None else '4G',
        wallets_file=wallets_file,
    )
    return container


def make_container_name(network_name: str, name: str) -> str:
    return "{network_name}.{name}".format(network_name=network_name, name=name)


def make_bootstrap_name(network_name: str) -> str:
    return make_container_name(network_name=network_name, name='bootstrap')


def make_peer_name(network_name: str, name: str) -> str:
    if name.isdigit():
        actual_name = 'peer{}'.format(name)
    else:
        actual_name = name
    return make_container_name(network_name=network_name, name=actual_name)


def make_peer(
    *,
    docker_client: DockerClient,
    network: str,
    name: str,
    bonds_file: str,
    command_timeout: int,
    bootstrap: Node,
    private_key: PrivateKey,
    allowed_peers: Optional[List[str]] = None,
    mem_limit: Optional[str] = None,
    wallets_file: Optional[str] = None,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0,
    max_peer_queue_size: int = 10,
    give_up_after_skipped: int = 0,
    drop_peer_after_retries: int = 0
) -> Node:
    assert isinstance(name, str)
    assert '_' not in name, 'Underscore is not allowed in host name'
    name = make_peer_name(network, name)

    bootstrap_address = bootstrap.get_rnode_address()

    container_command_flags = set([
        "--prometheus",
        "--no-upnp",
        "--allow-private-addresses"
    ])

    if cli_flags is not None:
        container_command_flags.update(cli_flags)

    container_command_options = {
        "--bootstrap":                      bootstrap_address,
        "--validator-private-key":          private_key.to_hex(),
        "--validator-public-key":           private_key.get_public_key().to_hex(),
        "--host":                           name,
        "--synchrony-constraint-threshold": synchrony_constraint_threshold,
        "--max-peer-queue-size":            max_peer_queue_size,
        "--give-up-after-skipped":          give_up_after_skipped,
        "--drop-peer-after-retries":        drop_peer_after_retries
    }

    if cli_options is not None:
        container_command_options.update(cli_options)

    container = make_node(
        docker_client=docker_client,
        name=name,
        network=network,
        bonds_file=bonds_file,
        container_command='run',
        container_command_flags=container_command_flags,
        container_command_options=container_command_options,
        command_timeout=command_timeout,
        extra_volumes=extra_volumes,
        allowed_peers=allowed_peers,
        mem_limit=mem_limit if not None else '4G',
        wallets_file=wallets_file,
    )
    return container


@contextlib.contextmanager
def started_peer(
    *,
    context: TestingContext,
    network: str,
    name: str,
    bootstrap: Node,
    private_key: PrivateKey,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0
) -> Generator[Node, None, None]:
    peer = make_peer(
        docker_client=context.docker,
        network=network,
        name=name,
        bonds_file=context.bonds_file,
        bootstrap=bootstrap,
        private_key=private_key,
        command_timeout=context.command_timeout,
        wallets_file=context.wallets_file,
        cli_flags=cli_flags,
        cli_options=cli_options,
        extra_volumes=extra_volumes,
        synchrony_constraint_threshold=synchrony_constraint_threshold
    )
    try:
        wait_for_node_started(context, peer)
        yield peer
    finally:
        peer.cleanup()


@contextlib.contextmanager
def bootstrap_connected_peer(
    *,
    context: TestingContext,
    bootstrap: Node,
    name: str,
    private_key: PrivateKey,
    cli_options: Optional[Dict[str, str]] = None,
    synchrony_constraint_threshold: float = 0.0
) -> Generator[Node, None, None]:
    with started_peer(
        context=context,
        network=bootstrap.network,
        name=name,
        bootstrap=bootstrap,
        private_key=private_key,
        cli_options=cli_options,
        synchrony_constraint_threshold=synchrony_constraint_threshold
    ) as peer:
        wait_for_approved_block_received_handler_state(context, peer)
        yield peer


def create_peer_nodes(
    *,
    docker_client: DockerClient,
    bootstrap: Node,
    network: str,
    bonds_file: str,
    private_keys: List[PrivateKey],
    command_timeout: int,
    allowed_peers: Optional[List[str]] = None,
    mem_limit: Optional[str] = None,
) -> List[Node]:
    assert len(set(private_keys)) == len(private_keys), "There shouldn't be any duplicates in the key pairs"

    if allowed_peers is None:
        allowed_peers = [bootstrap.name] + [make_peer_name(network, str(i)) for i in range(0, len(private_keys))]

    result = []
    try:
        for i, private_key in enumerate(private_keys):
            peer_node = make_peer(
                docker_client=docker_client,
                network=network,
                name=str(i),
                bonds_file=bonds_file,
                command_timeout=command_timeout,
                bootstrap=bootstrap,
                private_key=private_key,
                allowed_peers=allowed_peers,
                mem_limit=mem_limit if mem_limit is not None else '4G',
            )
            result.append(peer_node)
    except:
        for node in result:
            node.cleanup()
        raise
    return result


def make_random_network_name(context: TestingContext, length: int) -> str:
    return ''.join(context.random_generator.choice(string.ascii_lowercase) for m in range(length))


@contextlib.contextmanager
def docker_network(context: TestingContext, docker_client: DockerClient) -> Generator[str, None, None]:
    network_name = "rchain-{}".format(make_random_network_name(context, 5))
    docker_client.networks.create(network_name, driver="bridge")
    try:
        yield network_name
    finally:
        for network in docker_client.networks.list():
            if network_name == network.name:
                network.remove()


@contextlib.contextmanager
def started_bootstrap(
    *,
    context: TestingContext,
    network: str,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict[str, str]] = None,
    extra_volumes: Optional[List[str]] = None,
    synchrony_constraint_threshold: float = 0.0
) -> Generator[Node, None, None]:
    bootstrap_node = make_bootstrap_node(
        docker_client=context.docker,
        network=network,
        bonds_file=context.bonds_file,
        private_key=context.bootstrap_key,
        command_timeout=context.command_timeout,
        cli_flags=cli_flags,
        cli_options=cli_options,
        wallets_file=context.wallets_file,
        extra_volumes=extra_volumes,
        synchrony_constraint_threshold=synchrony_constraint_threshold
    )
    try:
        wait_for_node_started(context, bootstrap_node)
        yield bootstrap_node
    finally:
        bootstrap_node.cleanup()


@contextlib.contextmanager
def started_bootstrap_with_network(
    context: TestingContext,
    cli_flags: Optional[AbstractSet] = None,
    cli_options: Optional[Dict] = None,
    synchrony_constraint_threshold: float = 0.0,
    extra_volumes: Optional[List[str]] = None,
    wait_for_approved_block: bool = False,
) -> Generator[Node, None, None]:
    with docker_network(context, context.docker) as network:
        with started_bootstrap(
                context=context,
                network=network,
                cli_flags=cli_flags,
                cli_options=cli_options,
                synchrony_constraint_threshold=synchrony_constraint_threshold,
                extra_volumes=extra_volumes,
        ) as bootstrap:
            if wait_for_approved_block:
                wait_for_approved_block_received_handler_state(context, bootstrap)
            yield bootstrap

ready_bootstrap_with_network = functools.partial(started_bootstrap_with_network,
        wait_for_approved_block=True)
